{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwlxK5AYASaT",
        "outputId": "9d86e053-ccb2-4f56-d4cc-27d735206b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# \u0414\u0430\u043d\u043d\u044b\u0439 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435 google-colab\n",
        "%pip install catboost fasttext -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRUXhCVUzur"
      },
      "source": [
        "# \u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 \"NLP. \u0427\u0430\u0441\u0442\u044c 1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "outputs": [],
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "outputs": [],
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = ['<UNK>'] + sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eemkFZ1tVLw4"
      },
      "source": [
        "## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 1 (0.5 \u0431\u0430\u043b\u043b\u0430)\n",
        "\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c One-Hot \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0442\u0435\u043a\u0441\u0442\u043e\u0432"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "outputs": [],
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[List[int]]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    if vocab is None:\n",
        "        vocab = sorted(set(tokens))\n",
        "    if vocab_index is None:\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "    tokens_index = [vocab_index.get(token, 0) for token in tokens]\n",
        "    one_hot_vectors = np.eye(vocab_size, dtype=int)\n",
        "    return one_hot_vectors[tokens_index].tolist()\n",
        "\n",
        "def test_one_hot_vectorization(\n",
        "    vocab: List[str],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result[0]) != expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for i, word in enumerate(words_in_text):\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[i][idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2-LJcmbTe04",
        "outputId": "b5c49faf-5c9e-4c6d-9c34-daad35139bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-Hot-Vectors test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_one_hot_vectorization(vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAF8IOYMVT3s"
      },
      "source": [
        "## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 2 (0.5 \u0431\u0430\u043b\u043b\u0430)\n",
        "\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "outputs": [],
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "\n",
        "    counter = {}\n",
        "    for token in tokens:\n",
        "        counter[token] = counter.get(token, 0) + 1\n",
        "\n",
        "    return counter\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScFuXh_9TtJm",
        "outputId": "051d9a8c-690f-4939-c726-8440dd4bc101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bad-of-Words test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_bag_of_words_vectorization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LblWJfX2kr"
      },
      "source": [
        "## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 3 (0.5 \u0431\u0430\u043b\u043b\u0430)\n",
        "\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "outputs": [],
      "source": [
        "def tf_idf_vectorization(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[float]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "\n",
        "    if corpus is None:\n",
        "        corpus = [text]\n",
        "    if vocab is None:\n",
        "        vocab, _ = build_vocab(corpus)\n",
        "    if vocab_index is None:\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    token_counts = Counter(tokens)\n",
        "    count_tokens = len(tokens)\n",
        "    tf_vector = np.array([token_counts.get(token, 0) / count_tokens for token in vocab])\n",
        "\n",
        "    num_docs = len(corpus)\n",
        "    doc_frequency = np.zeros(vocab_size)\n",
        "\n",
        "    for doc in corpus:\n",
        "        doc_tokens = set(normalize_pretokenize_text(doc))\n",
        "        for i, token in enumerate(vocab):\n",
        "            if token in doc_tokens:\n",
        "                doc_frequency[i] += 1\n",
        "\n",
        "    idf_vector = np.log(num_docs / (doc_frequency + 1))\n",
        "    tf_idf_vector = tf_vector * idf_vector\n",
        "\n",
        "    return tf_idf_vector.tolist()\n",
        "\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown\"\n",
        "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"TF-IDF test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKIyS724T0XH",
        "outputId": "d7f4007f-d171-4797-8dd9-49282c3df64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0f9FZCrX5_s"
      },
      "source": [
        "## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 4 (1 \u0431\u0430\u043b\u043b)\n",
        "\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
        "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
        "\u0433\u0434\u0435 $N(word, context)$ -- \u0447\u0438\u0441\u043b\u043e \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0439 \u0441\u043b\u043e\u0432\u0430 $word$ \u0432 \u043e\u043a\u043d\u043e $context$ (\u0440\u0430\u0437\u043c\u0435\u0440 \u043e\u043a\u043d\u0430 -- \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "outputs": [],
      "source": [
        "def ppmi_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> List[float]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "\n",
        "    if corpus is None:\n",
        "        corpus = [text]\n",
        "    if vocab is None:\n",
        "        vocab, _ = build_vocab(corpus)\n",
        "    if vocab_index is None:\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "    corpus_stats = defaultdict(int)\n",
        "    word_counts = Counter()\n",
        "    total_pairs = 0\n",
        "\n",
        "    for doc in corpus:\n",
        "        doc_tokens = normalize_pretokenize_text(doc)\n",
        "        for i, word in enumerate(doc_tokens):\n",
        "            word_counts[word] += 1\n",
        "\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(doc_tokens), i + window_size + 1)\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context = doc_tokens[j]\n",
        "                    corpus_stats[(word, context)] += 1\n",
        "                    total_pairs += 1\n",
        "\n",
        "    ppmi_vector = [0.0] * vocab_size\n",
        "\n",
        "    for token in set(tokens):\n",
        "        for i, context_word in enumerate(vocab):\n",
        "            n_word_context = corpus_stats[(token, context_word)]\n",
        "\n",
        "            if n_word_context == 0:\n",
        "                continue\n",
        "\n",
        "            n_word = word_counts[token]\n",
        "            n_context = word_counts[context_word]\n",
        "\n",
        "            pmi = math.log((n_word_context * total_pairs) / (n_word * n_context))\n",
        "            ppmi = max(0, pmi)\n",
        "\n",
        "            ppmi_vector[i] += ppmi\n",
        "\n",
        "    return ppmi_vector\n",
        "\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgHmNZy75XFV",
        "outputId": "2edf3836-9fbf-4c54-c9e7-381745f0b211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPMI test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK29va3PBH_8"
      },
      "source": [
        "## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 5 (1 \u0431\u0430\u043b\u043b)\n",
        "\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0438\u0437 fasttext \u0438 bert (\u0434\u043b\u044f bert \u043b\u0443\u0447\u0448\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c CLS \u0442\u043e\u043a\u0435\u043d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2cOKbo-5SUL",
        "outputId": "91cd6e49-0b5e-4100-87ef-ed7842e9d7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        }
      ],
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "fasttext_model = fasttext.load_model('cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tOe8dRLl5eqN"
      },
      "outputs": [],
      "source": [
        "def get_fasttext_embeddings(text: str, model: any = None) -> List[np.ndarray]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    embeddings = [model.get_word_vector(token) for token in tokens]\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A9GXy6n0AtsZ"
      },
      "outputs": [],
      "source": [
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    pool_method: str = 'cls'\n",
        ") -> np.ndarray:\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer(text, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    embedding = last_hidden_state[:, 0, :].squeeze()\n",
        "\n",
        "    return embedding.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KoKolrD49R"
      },
      "source": [
        "## \u0417\u0430\u0434\u0430\u043d\u0438\u0435 6 (1.5 \u0431\u0430\u043b\u043b\u0430)\n",
        "\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043f\u043e\u0432\u0435\u0440\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432, \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0432 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u0437\u0430\u0434\u0430\u043d\u0438\u044f\u0445, \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043a\u0430\u043a\u0443\u044e-\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c (\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u043d\u0435\u0433\u043b\u0443\u0431\u043e\u043a\u0443\u044e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, CatBoost) \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0412\u044b\u043d\u0435\u0441\u0435\u043c \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432 \u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0437\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u044b \u0444\u0443\u043d\u043a\u0446\u0438\u0438 vectorize_texts - \u044d\u0442\u043e \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0442 \u043f\u0430\u043c\u044f\u0442\u044c, \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043f\u043e\u0447\u0442\u0438 \u043d\u0435 \u0445\u0432\u0430\u0442\u0430\u0435\u0442, \u0435\u0441\u043b\u0438 \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u0430\u0442\u0447\u0438. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "43d7f2689830442187d50b5fe58a4f7d",
            "11a98531add645eba90794ffbd2b5ae4",
            "dd3594c5b3404b01bb7af69f4dd7af3f",
            "89c3a820977b479c8bedb5459c556c83",
            "2d45f39b032440d7978b5c8245f87c70",
            "05324dd37fff4746bfbbbc05efa02151",
            "9376c78162734a58b8c9a77d5809c3fa",
            "0f83fa8a77ab4c53a770bc2f209a47a4",
            "21a0e169bc504dea84e120fdc22ae1bd",
            "f9c5370ffe614a34a0a82d96994efe6f",
            "0c0d77a07fda4422a2ceaa478f1efb25",
            "524d20868a594ec3ab8ca2ce5a41ef5a",
            "05433f8eb15948a995a4ec164c905c7f",
            "887b568982fd4ad6a2acf59318299d97",
            "2ad0d07e00814e2f8877cc12a4b3f8f9",
            "a76e752cdb0f4616b0f14f52b7271dd6",
            "20e4a9840dab41779cf3ef54c34a8372",
            "a1dbd3e7851048f29aeb3b224e4482bc",
            "f28f93aa23c141beba2c776d92475e10",
            "4d4a0b78f91b44a082d73531cbfd890d",
            "e63c70f002b6460eaa19480badaa306b",
            "8b221bfceccb43ebb1ff59f82276c837",
            "d23dfe7867444c9dbae4cb0e8a711d9d",
            "13e7a4b12c844e0db467c36cf765248a",
            "c8fd6fbdcd494bd8aa3c391ae3855634",
            "a84b5d3bc78e4f7ea56bacbc83947596",
            "f5b7c35becaa454db04093c9cb8b24b8",
            "9169d3297b9c44769189428d4a2f446c",
            "0e1022b8a2c34612a892f4d3c2d8b6a8",
            "4ce0ff4a1f984012851c99a456b3abdc",
            "adcde701129a4343bd64f5c188a95565",
            "c2c5dcbebb3841fbb1f52e76d57becf3",
            "b2f30e07946c4f1286ef2575c01f1e93"
          ]
        },
        "id": "mdjGrvn0TxcT",
        "outputId": "03642097-c512-4c35-9454-8ba4e4f3f845"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43d7f2689830442187d50b5fe58a4f7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "524d20868a594ec3ab8ca2ce5a41ef5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d23dfe7867444c9dbae4cb0e8a711d9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_dataset = datasets.load_dataset(\"imdb\", split=\"train\")\n",
        "test_dataset = datasets.load_dataset(\"imdb\", split=\"test\")\n",
        "\n",
        "train_dataset = train_dataset.shuffle(seed=42).select(range(100))\n",
        "test_dataset = test_dataset.shuffle(seed=42).select(range(100))\n",
        "\n",
        "train_texts = [item['text'] for item in train_dataset]\n",
        "train_labels = [item['label'] for item in train_dataset]\n",
        "test_texts = [item['text'] for item in test_dataset]\n",
        "test_labels = [item['label'] for item in test_dataset]\n",
        "\n",
        "vocab, vocab_index = build_vocab(train_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0414\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0432\u043c\u0435\u0441\u0442\u043e one-hot \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c multi-hot. \u041f\u043e \u0441\u0443\u0442\u0438 \u044d\u0442\u043e \u0442\u0430\u043a\u043e\u0439 \u0436\u0435 one-hot, \u043d\u043e \u0432\u043c\u0435\u0441\u0442\u043e \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0431\u0443\u0434\u0435\u043c \u0432\u044b\u0434\u0430\u0432\u0430\u0442\u044c \u043e\u0434\u043d\u0443 \u0441\u0442\u0440\u043e\u043a\u0443 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 vocab_size, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0441\u0442\u043e\u0438\u0442 1 \u0441\u0442\u043e\u044f\u0442 \u043d\u0430 \u043f\u043e\u0437\u0438\u0446\u0438\u044f\u0445 \u0442\u043e\u043a\u0435\u043d\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0441\u0442\u044c \u0432 \u0442\u0435\u043a\u0441\u0442\u0435. \u0421\u0434\u0435\u043b\u0430\u043d\u043e \u044d\u0442\u043e \u0431\u044b\u043b\u043e \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0440\u0435\u0448\u0438\u0442\u044c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0440\u0430\u0437\u043d\u044b\u0445 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0435\u0439 one-hot \u043c\u0430\u0442\u0440\u0438\u0446, \u0442.\u043a. \u0442\u0435\u043a\u0441\u0442\u044b, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0435 \u0440\u0430\u0437\u043d\u043e\u0435 \u043a\u043e\u043b-\u0432\u043e \u0441\u043b\u043e\u0432, \u0431\u0443\u0434\u0443\u0442 \u0438\u043c\u0435\u0442\u044c \u0440\u0430\u0437\u043d\u043e\u0435 \u043a\u043e\u043b-\u0432\u043e \u0441\u0442\u0440\u043e\u043a \u0432 one-hot \u043c\u0430\u0442\u0440\u0438\u0446\u0435. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tKN0BxYH4tJN"
      },
      "outputs": [],
      "source": [
        "def multi_hot_vectorization(text, vocab, vocab_index):\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    multi_hot_vector = [0] * vocab_size\n",
        "\n",
        "    for token in tokens:\n",
        "        idx = vocab_index.get(token, 0)\n",
        "        multi_hot_vector[idx] = 1\n",
        "\n",
        "    return multi_hot_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zCrp7DdhUCR7"
      },
      "outputs": [],
      "source": [
        "def vectorize_texts(texts, vectorizer_type):\n",
        "    vectorized_data = []\n",
        "\n",
        "    for text in texts:\n",
        "        if vectorizer_type == \"one_hot\":\n",
        "            vectorized_data.append(multi_hot_vectorization(text, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"bow\":\n",
        "            bow_dict = bag_of_words_vectorization(text)\n",
        "            vector = [bow_dict.get(word, 0) for word in vocab]\n",
        "            vectorized_data.append(vector)\n",
        "        elif vectorizer_type == \"tfidf\":\n",
        "            vectorized_data.append(tf_idf_vectorization(text, texts, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"ppmi\":\n",
        "            vectorized_data.append(ppmi_vectorization(text, texts, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"fasttext\":\n",
        "            embeddings = get_fasttext_embeddings(text, fasttext_model)\n",
        "            if embeddings:\n",
        "                avg_embedding = np.mean(embeddings, axis=0)\n",
        "                vectorized_data.append(avg_embedding.tolist())\n",
        "            else:\n",
        "                vectorized_data.append([0] * 300)\n",
        "        elif vectorizer_type == \"bert\":\n",
        "            embedding = get_bert_embeddings(text)\n",
        "            vectorized_data.append(embedding.tolist())\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
        "\n",
        "    return vectorized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DRRw01XiBg6H"
      },
      "outputs": [],
      "source": [
        "def train(embeddings_method):\n",
        "    print(f\"{embeddings_method} start.\")\n",
        "    X = vectorize_texts(train_texts, embeddings_method)\n",
        "    X_test = vectorize_texts(test_texts, embeddings_method)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(train_labels)\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=500,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        verbose=500,\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "\n",
        "    print(f\"{embeddings_method} fit.\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"{embeddings_method}: test accuracy={test_acc}, test f1={test_f1}\")\n",
        "\n",
        "    del X_train, X_val, y_train, y_val, y_test_pred\n",
        "\n",
        "    return test_acc, test_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698,
          "referenced_widgets": [
            "11df7531904546c3aac416aeeb575172",
            "d023095f41ff478fac443687c53d7a37",
            "dedf169944314ff690964ebf3df3ccd2",
            "adef0037c875406fac1e81ee8eb23de2",
            "459eb8e6cb1448ec86bcb22fe36abdbe",
            "7027b3535b6544eb8b9a6daf8da8a94f",
            "d28c6b960dd549009c9b17a00b69ffd2",
            "650350b2b4364290aa08d9cab9884fd6",
            "c377ea913f5b47cab064ea73d8bea823",
            "4f21f289d8ef42169804407275a06270",
            "1212b93ceeef40f4bc42b0fd48a94c56",
            "f77f3548054148659d54cd5d6ee61915",
            "7670294e38414ef39a89a63ec2755745",
            "2f30abebddc846a79f7014dcd67365f0",
            "cb114c6edbf5447797dfb5ed37fdecf3",
            "8f43835f5891458f924efff24989731a",
            "442bb59b3e1b4506ab742c6b658a4b43",
            "771c0d548bb04c27ad60291f860c4454",
            "d6b6d0b43ed44d30b3d335770d4a3ac9",
            "084b7ed8c85148caab8da65b0ce87ee8",
            "3da8d8bfc95c496ebcff8e9a9ec958d5",
            "c695df66d5c64ff0bbac025bbcf80721",
            "35546ab918be463e8ded6d9a97ef75f2",
            "b06a077f33fe408785385ba0869cbbba",
            "753fac0c8cba42608fbc62ef6fd91c80",
            "cfd103e769e24853810cff9e4a100c6e",
            "b0b6f06ef7fa45a494adb85af0b75753",
            "b7ede19045b3430caa8a6316d71f9a9c",
            "df4933b4f6a74337a682ded48eef8daf",
            "27d671d08a39498f907c01623ca3b8fe",
            "899abfb7eced475b8ba0f9329404f907",
            "0916a619660d490589b6212ea1764858",
            "7dacada89e134440a4b336505579f928",
            "4d04000bf1cc4fb5b4ded19f40eb3df7",
            "75032e43eb634af78bb15fecd2a4dff4",
            "82c55a8352ba4230a8e3c25e6602ab66",
            "5978ef4e1857466c857800ee6693f665",
            "85915e9acfbb48b5a6fee43b355d3b70",
            "46528f4f1bd74f3b9c661da0787aef51",
            "88a1d8e16668495196c143fb79fd805c",
            "19a19b9a7e6d49ba9b39cf0b3710249a",
            "25e7b12b2fff4864b64861fba60152e5",
            "9c04ef1069bf4f4dbfec355ae6220c62",
            "6954cf9161114cd7bf847ec8f8100104",
            "6b9d00d4b7da462384fcf632ff546e3e",
            "c854849b68b04c1390e922cc1355247a",
            "cbd9d1313ad343e8bbf3a29818b32cd8",
            "7b44ee7a05644e8a88b662a67c39958e",
            "713a72c910974392aa650e6885f2a750",
            "a4f146df7f904aa6ae9c4f21b462d3e8",
            "051acb700fa14cb9b69041f700879dd9",
            "aae2b7cdc71f4308bc4a3fb63cc8a612",
            "851fd7d9c2ae480288661beb42ff33db",
            "20414d7c52004082b9fe8008b80cd86a",
            "a9daa67c678c458aa47145a9462602da"
          ]
        },
        "id": "naMqAkjqFHAe",
        "outputId": "3c8193ef-2951-4896-d935-99f16930594f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bow start.\n",
            "bow fit.\n",
            "0:\tlearn: 0.6428838\ttotal: 41.2ms\tremaining: 20.6s\n",
            "499:\tlearn: 0.0016805\ttotal: 12.8s\tremaining: 0us\n",
            "bow: test accuracy=0.6, test f1=0.5918367346938775\n",
            "one_hot start.\n",
            "one_hot fit.\n",
            "0:\tlearn: 0.6566856\ttotal: 25.5ms\tremaining: 12.7s\n",
            "499:\tlearn: 0.0020489\ttotal: 13s\tremaining: 0us\n",
            "one_hot: test accuracy=0.63, test f1=0.6105263157894737\n",
            "tfidf start.\n",
            "tfidf fit.\n",
            "0:\tlearn: 0.6621224\ttotal: 31.6ms\tremaining: 15.8s\n",
            "499:\tlearn: 0.0016805\ttotal: 16.5s\tremaining: 0us\n",
            "tfidf: test accuracy=0.64, test f1=0.5714285714285714\n",
            "ppmi start.\n",
            "ppmi fit.\n",
            "0:\tlearn: 0.6348993\ttotal: 200ms\tremaining: 1m 39s\n",
            "499:\tlearn: 0.0015972\ttotal: 1m 25s\tremaining: 0us\n",
            "ppmi: test accuracy=0.52, test f1=0.6521739130434783\n",
            "fasttext start.\n",
            "fasttext fit.\n",
            "0:\tlearn: 0.6276676\ttotal: 35.3ms\tremaining: 17.6s\n",
            "499:\tlearn: 0.0018762\ttotal: 14.4s\tremaining: 0us\n",
            "fasttext: test accuracy=0.63, test f1=0.5934065934065934\n",
            "bert start.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11df7531904546c3aac416aeeb575172",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f77f3548054148659d54cd5d6ee61915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35546ab918be463e8ded6d9a97ef75f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d04000bf1cc4fb5b4ded19f40eb3df7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b9d00d4b7da462384fcf632ff546e3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert fit.\n",
            "0:\tlearn: 0.6287975\ttotal: 172ms\tremaining: 1m 25s\n",
            "499:\tlearn: 0.0011828\ttotal: 36.9s\tremaining: 0us\n",
            "bert: test accuracy=0.64, test f1=0.64\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "for embeddings_method in [\"bow\", \"one_hot\", \"tfidf\", \"ppmi\", \"fasttext\", \"bert\"]:\n",
        "    acc, f1 = train(embeddings_method)\n",
        "    results[embeddings_method] = {'accuracy': acc, 'f1': f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDxYwSO30aq",
        "outputId": "074e2e07-2a75-4e08-fb4c-00db936aef5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bow': {'accuracy': 0.6, 'f1': 0.5918367346938775}, 'one_hot': {'accuracy': 0.63, 'f1': 0.6105263157894737}, 'tfidf': {'accuracy': 0.64, 'f1': 0.5714285714285714}, 'ppmi': {'accuracy': 0.52, 'f1': 0.6521739130434783}, 'fasttext': {'accuracy': 0.63, 'f1': 0.5934065934065934}, 'bert': {'accuracy': 0.64, 'f1': 0.64}}\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS9kd-kr30aq"
      },
      "source": [
        "### \u0412\u044b\u0432\u043e\u0434\u044b\n",
        "\u0425\u043e\u0442\u044c \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u043a\u043e\u0440\u043f\u0443\u0441\u0435 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0438 \u043d\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0438\u0437-\u0437\u0430 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0434\u043e\u043b\u0433\u043e\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043d\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u044b\u0435. \u041d\u043e \u0434\u0430\u0436\u0435 \u043d\u0430 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u043c \u043a\u043e\u0440\u043f\u0443\u0441\u0435 \u0432\u0438\u0434\u043d\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e bert \u043f\u0435\u0440\u0435\u0434 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u0432\u0438\u0434\u0430\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438. \u0414\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0434\u043e\u043b\u0433\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u044e\u0442\u0441\u044f tf-idf \u0438 ppmi. bow \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u0435\u0442\u0441\u044f \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e. bert \u0438 fasttext \u043a\u0430\u043a \u0431\u0443\u0434\u0442\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u044e\u0442\u0441\u044f \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u0447\u0435\u043c tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}